# coding=utf-8
import sys
import time

sys.path.append("..")
from bs4 import BeautifulSoup
from summa.summarizer import summarize
from requests_html import HTMLSession
from concurrent.futures import ThreadPoolExecutor
from loguru import logger
import json
import os
import sys
import requests
import Levenshtein

pubs_pool = ThreadPoolExecutor(5)
session = HTMLSession()

end_lists_doc = json.load(open(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'docs/end_words.json')))


def get_abstract(url):
    try:
        r = session.get(url)
    except Exception as e:
        exc_type, exc_value, exc_obj = sys.exc_info()
        logger.error("exception_type: %s,exception_value: %s,exception_object: %s," % (
            str(exc_type), str(exc_value), str(exc_obj)))
        return "", ""
    a = r.html.text.split('\n')  # r.html.textå¯å»æ‰htmlä¸­çš„æ‰€æœ‰æ ‡ç­¾
    flag = False  # ä½¿ç”¨flagæ¥ç¡®å®šæ˜¯å¦ä¸ºéœ€å†™å…¥çš„å†…å®¹
    lists = ['æ¥æºï¼š', 'é˜…è¯»åŸæ–‡', 'å–œæ¬¢æœ¬ç¯‡å†…å®¹', 'åˆ†äº«ã€ç‚¹èµ', 'åŸæ–‡é“¾æ¥', 'æ”¶å½•äºè¯é¢˜', 'ç‚¹å‡»ä¸Šæ–¹', 'å¾®ä¿¡å¹³å°', 'å‘è‡ª', 'å…¬ä¼—å·',
             'ç‚¹å‡»ä¸‹æ–¹', 'ç‚¹å‡»ç›´è¾¾', 'é•¿æŒ‰è¯†åˆ«', 'ğŸ‘‡', 'æ‰«æä¸‹æ–¹', 'æœ¬æœŸè´¡çŒ®è€…', 'äºŒç»´ç ', 'ä½œè€…', 'è´£ç¼–', 'è®°è€…', "è”ç³»é‚®ç®±"]
    alllist = []
    allstr = ""
    title_list = get_inbound_link_title(r.html.links)
    # title_list=[]
    for num in range(len(a)):
        if 'åŠŸèƒ½ä»‹ç»' in a[num]:  # å†™å…¥â€˜åŠŸèƒ½ä»‹ç»â€™ä»¥åçš„å†…å®¹
            flag = True
            continue
        elif 'var first_sceen__time' in a[num]:  # å†™å…¥â€˜var first_sceen__timeâ€™ä»¥å‰çš„å†…å®¹
            flag = False
        elif flag:
            if any(i in a[num] for i in lists):
                continue
            if a[num] == '':
                continue
            # åˆ é™¤å¤–é“¾
            del_inbound_flag = False
            for title in title_list:
                ratio = Levenshtein.ratio(title, a[num])
                if ratio > 0.8:
                    del_inbound_flag = True
                    break
            if del_inbound_flag:
                continue
            alllist.append(a[num])
    end_words = ["æŠ•ç¨¿æ¨¡æ¿", "ç›¸å…³é˜…è¯»", "é˜…è¯»åŸæ–‡", "è¿˜å–œæ¬¢", "å†™åœ¨æœ€å", "ç›¸å…³è¿›å±•", "å‚è€ƒèµ„æ–™", "æ¥æº", "ç¼–è¾‘", "å®¢æœ", "å¾€æœŸç²¾å½©", "ç¤¾ä¼šè´£ä»»ä¹‹å£°", "ç”±é’å¹´å”±å“å…¨çƒ",
                 "åœ¨çœ‹", "ç´ æ", "æŠ•ç¨¿", "ç½‘ç«™", "åˆ†äº«", "å…³æ³¨", "è”ç³»", "æ„Ÿè°¢", "é™æ—¶ä¼˜æƒ ", "è®¢é˜…ç‚¹å‡»", "è”ç³»äºº", "å¾®ä¿¡ç¾¤", "æ‰«ä¸€æ‰«", "æ¥æº", "å›é¡¾", "æ–‡/",
                 "å›¾/", "çŒœä½ å–œæ¬¢"]
    end_lists = {word: 0.1 for word in end_words}

    for end_word, weight in end_lists_doc.items():
        if weight > 0.1:
            end_lists[end_word] = weight
        else:
            end_lists[end_word] = 0.1
    # end_lists = {"ç›¸å…³è¿›å±•": 1, "å†™åœ¨æœ€å": 1, "è¿˜å–œæ¬¢": 1, "é˜…è¯»åŸæ–‡": 1, "ç›¸å…³é˜…è¯»": 1, "æŠ•ç¨¿æ¨¡æ¿": 1}
    total_length = len("".join(alllist))
    now_length = 0
    for i in range(len(alllist)):
        paragraph = alllist[i]
        in_end = [word in paragraph for word in end_lists.keys()]
        if any(in_end):
            end_word = list(end_lists.keys())[in_end.index(True)]
            if len("".join(alllist[i:])) / total_length < end_lists[end_word]:
                # print(end_word, len("".join(alllist[i:])) / total_length)
                now_length += 1
                break
        allstr += paragraph + '\n'

    text = allstr

    content = summarize(text, ratio=0.3, split=False)

    try:
        if alllist[0] not in content:
            content = alllist[0] + '\n' + content
    except:
        pass
    abstract = ''.join(content)
    return abstract, text


# è·å–å¤–é“¾çš„anchorï¼Œè¿™é‡Œä½¿ç”¨titleä»£æ›¿anchor
def get_inbound_link_title(link):
    title_list = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'
    }
    for i, net_link in enumerate(link):
        # print(net_link)
        html = requests.get(net_link, headers=headers, timeout=3).text
        soup = BeautifulSoup(html, 'html.parser')
        # print(result)
        og_title = soup.find("meta", property="og:title")
        if og_title != None:
            title = og_title['content']
            title_list.append(title)
    return title_list


if __name__ == "__main__":
    pass
    start = time.time()
    abstract, content = get_abstract("https://blog.csdn.net/weixin_41894030/article/details/115264687")
    # print(abstract)
    print(" ".join(content.split()))
    print(time.time() - start)
